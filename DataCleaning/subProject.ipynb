{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Sub-project\n",
    " Created by Christopher Maher\n",
    "### Goal: Create a API cleaning pipeline\n",
    "\n",
    "  This project is to create an API cleaning ETL pipeline that allows a given API to be cleaned based on the following criteria:\n",
    "\n",
    "  * Handling Missing Data\n",
    "  * Dealing with Duplicate Data\n",
    "  * Standardization of Data\n",
    "  * Handling Outliers of Data\n",
    "  * Reshaping Data \n",
    "  * Filtering and Selecting Data\n",
    "\n",
    "This data pipeline will also attempt to fit best practices in ETL pipeline building which means the following:\n",
    "\n",
    "- Error handling\n",
    "- Logging\n",
    "- Testing\n",
    "\n",
    "The pipeline will be built with an attempt to be able to scale however due to the nature of the project will be unneeded and will be an attempt at best practice.\n",
    "\n",
    "#### Project Details\n",
    "\n",
    "  Use the Spotify API to identify trends in users 'saved songs', more commonly known as favorite songs, to identify their preferred listening type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports and variables\n",
    "All imports are listed below as well as versions used.\n",
    "\n",
    "Versions developed on:\n",
    "\n",
    " `pandas: 2.0.0`\n",
    "\n",
    " `logging: 0.5.1.2`\n",
    " \n",
    " `requests: 2.28.2`\n",
    " \n",
    " `json: 2.0.9`\n",
    "\n",
    " `numpy: 1.24.2`\n",
    "\n",
    "Check to current versions in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import time\n",
    "import json\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import spotipy\n",
    "import scipy.stats as stats\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "## .gitignore file containing secure information not to be published, stores as clientID and clientSecret both as strings\n",
    "import keys\n",
    "\n",
    "# Debug\n",
    "DEBUG = False\n",
    "\n",
    "# Proof of learning\n",
    "\n",
    "PL = False\n",
    "\n",
    "#Check versions\n",
    "if DEBUG:\n",
    "    print(pd.__version__)\n",
    "    print(logging.__version__)\n",
    "    print(requests.__version__)\n",
    "    print(json.__version__)\n",
    "    print(np.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval\n",
    "Retrives the data from the following API: Spotify API\n",
    "\n",
    "- This API is a large data collection provided by Spotify that allows multiple data features to be collected. This can range from how music 'feels' based on internal classifications on Spotify to other similar information provided by the API\n",
    "- This API **requires** creditentials so it's important to add your own credientials into it in the variables listed above\n",
    "- Alongside that currently \n",
    "\n",
    "Puts the data into pandas Dataframes for future processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  album_group album_type                                            artists   \n",
      "0      single     single  [{'external_urls': {'spotify': 'https://open.s...  \\\n",
      "1       album      album  [{'external_urls': {'spotify': 'https://open.s...   \n",
      "2      single     single  [{'external_urls': {'spotify': 'https://open.s...   \n",
      "3       album      album  [{'external_urls': {'spotify': 'https://open.s...   \n",
      "4      single     single  [{'external_urls': {'spotify': 'https://open.s...   \n",
      "\n",
      "                                   available_markets   \n",
      "0                                                 []  \\\n",
      "1  [AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, B...   \n",
      "2  [AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, B...   \n",
      "3  [AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, B...   \n",
      "4  [AD, AE, AG, AL, AM, AO, AR, AT, AU, AZ, BA, B...   \n",
      "\n",
      "                                       external_urls   \n",
      "0  https://open.spotify.com/album/3qI7YPn8H0ukCpO...  \\\n",
      "1  https://open.spotify.com/album/42cH7mrkfljkqkx...   \n",
      "2  https://open.spotify.com/album/2MUnPlYdNix2siW...   \n",
      "3  https://open.spotify.com/album/5n1cTgcLYpugGWH...   \n",
      "4  https://open.spotify.com/album/7IWXZKQU9nUC3ae...   \n",
      "\n",
      "                                                href                      id   \n",
      "0  https://api.spotify.com/v1/albums/3qI7YPn8H0uk...  3qI7YPn8H0ukCpOzlJS3jh  \\\n",
      "1  https://api.spotify.com/v1/albums/42cH7mrkfljk...  42cH7mrkfljkqkxA2Ip9Xq   \n",
      "2  https://api.spotify.com/v1/albums/2MUnPlYdNix2...  2MUnPlYdNix2siWwPa22eo   \n",
      "3  https://api.spotify.com/v1/albums/5n1cTgcLYpug...  5n1cTgcLYpugGWHNQJqAba   \n",
      "4  https://api.spotify.com/v1/albums/7IWXZKQU9nUC...  7IWXZKQU9nUC3ae508E46p   \n",
      "\n",
      "                                              images           name   \n",
      "0  [{'height': 640, 'url': 'https://i.scdn.co/ima...    So Innocent  \\\n",
      "1  [{'height': 640, 'url': 'https://i.scdn.co/ima...  Be the Cowboy   \n",
      "2  [{'height': 640, 'url': 'https://i.scdn.co/ima...      September   \n",
      "3  [{'height': 640, 'url': 'https://i.scdn.co/ima...  forest avenue   \n",
      "4  [{'height': 640, 'url': 'https://i.scdn.co/ima...   Adult Movies   \n",
      "\n",
      "  release_date release_date_precision  total_tracks   type   \n",
      "0   2023-02-22                    day             1  album  \\\n",
      "1   2018-08-17                    day            14  album   \n",
      "2         2009                   year             1  album   \n",
      "3   2022-02-04                    day            12  album   \n",
      "4   2021-08-12                    day             5  album   \n",
      "\n",
      "                                    uri  \n",
      "0  spotify:album:3qI7YPn8H0ukCpOzlJS3jh  \n",
      "1  spotify:album:42cH7mrkfljkqkxA2Ip9Xq  \n",
      "2  spotify:album:2MUnPlYdNix2siWwPa22eo  \n",
      "3  spotify:album:5n1cTgcLYpugGWHNQJqAba  \n",
      "4  spotify:album:7IWXZKQU9nUC3ae508E46p  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a function to extract the first element of the dictionary\n",
    "def extract_first_val(row):\n",
    "    return list(row.values())[0]\n",
    "\n",
    "#Connect to Spotify API as a user\n",
    "try: \n",
    "    sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=keys.clientID,client_secret=keys.clientSecret,redirect_uri='http://localhost:8080'))\n",
    "    logging.info(\"Established API connection\")\n",
    "\n",
    "except :\n",
    "    print(\"Failed to connect to Spotify\")\n",
    "    logging.exception(\"Failed to connect to Spotify API\")\n",
    "    SystemExit\n",
    "\n",
    "\n",
    "results = sp.current_user_saved_tracks(limit=50)\n",
    "\n",
    "#Fun check for if I'm getting results here\n",
    "if DEBUG:\n",
    "    for idx, item in enumerate(results['items']):\n",
    "        track = item['track']\n",
    "        print(idx, track['artists'][0]['name'], \" â€“ \", track['name'])\n",
    "\n",
    "#put the data in dataframe! During this stage I 'unpack' a lot of the information since it's hidden in dictionaries\n",
    "timeAdded = pd.DataFrame(results['items']).drop(['track'],axis=1)\n",
    "\n",
    "trackInfo = pd.DataFrame(pd.DataFrame(results['items'])['track'].apply(lambda x: pd.Series(x)))\n",
    "\n",
    "albumInfo = trackInfo['album'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "#This was a literal pain to figure out, the for some reason changed their entry types to be a list for artists(from a series) then apply my lambda\n",
    "artistInfo = pd.DataFrame(albumInfo['artists'].tolist())[0].apply(lambda x: pd.Series(x))\n",
    "\n",
    "# apply the function to each row of the 'external_urls' column\n",
    "artistInfo['external_urls'] = artistInfo['external_urls'].apply(extract_first_val)\n",
    "\n",
    "albumInfo['external_urls'] = albumInfo['external_urls'].apply(extract_first_val)\n",
    "\n",
    "#Dropping columns I don't need\n",
    "\n",
    "print(albumInfo.head())\n",
    "#print(type(artistInfo['id'][49]))\n",
    "logging.info(\"Data has been entered into DataFrame\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Data Schemas\n",
    "   This is to allow us to better define what values we're expecting for each column that we created previously we're going to keep them individually now since it allows for easier cleaning, the important bit will be keeping a unique id on them which will be their row #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data passed schemas!\n"
     ]
    }
   ],
   "source": [
    "from schema import Schema\n",
    "error = False\n",
    "#Convert the artistInfo DataFrame to a dictionary so the schema can work. Then create and validate from the schema information\n",
    "dict_artistInfo = artistInfo.to_dict()\n",
    "artist_schema = Schema({'external_urls':dict[int,str],\n",
    "                            'href': dict[int,str],\n",
    "                            'id': dict[int,str],\n",
    "                            'name': dict[int,str],\n",
    "                            'type': dict[int,str],\n",
    "                            'uri': dict[int,str]\n",
    "})\n",
    "try:\n",
    "    artist_schema.validate(dict_artistInfo)\n",
    "except:\n",
    "    error = True\n",
    "    print(\"The artist has missing or wrong values\")\n",
    "    logging.error(\"Artist information doesn't have correct values or is missing values\")\n",
    "#Convert the albumInfo DataFrame to a dictionary so the schema can work. Then create and validate from the schema information\n",
    "dict_albumInfo = albumInfo.to_dict()\n",
    "album_schema = Schema({'album_group':dict[int,str],\n",
    "                        'album_type': dict[int,str],\n",
    "                        'artists': dict[int,str],\n",
    "                        'available_markets': dict[int,list],\n",
    "                        'external_urls': dict[int,str],\n",
    "                        'href': dict[int,str],\n",
    "                        'id': dict[int,str],                        \n",
    "                        'images': dict[int,list],\n",
    "                        'name': dict[int,str],\n",
    "                        'release_date': dict[int,str],\n",
    "                        'release_date_precision': dict[int,str],\n",
    "                        'total_tracks' : dict[int,str],\n",
    "                        'type': dict[int,str],\n",
    "                        'uri': dict[int,str]\n",
    "                        \n",
    "})\n",
    "try:\n",
    "    album_schema.validate(dict_albumInfo)\n",
    "except:\n",
    "    error = True\n",
    "    print(\"The album has missing or wrong values\")\n",
    "    logging.error(\"Album information doesn't have correct values or is missing values\")\n",
    "#Finally repeat for track\n",
    "dict_trackInfo = trackInfo.to_dict()\n",
    "track_schema = Schema({'album':dict[int,list],\n",
    "                        'artists': dict[int,dict],\n",
    "                        'available_markets': dict[int,list],\n",
    "                        'disc_number': dict[int,int],\n",
    "                        'duration_ms': dict[int,int],\n",
    "                        'explicit':dict[int,bool],\n",
    "                        'external_ids':dict[int,list],\n",
    "                        'external_urls':dict[int,str],\n",
    "                        'href': dict[int,str],\n",
    "                        'id': dict[int,str], \n",
    "                        'is_local': dict[int,bool],\n",
    "                        'name': dict[int,str],\n",
    "                        'popularity':dict[int,int],\n",
    "                        'preview_url':dict[int,str],\n",
    "                        'track_number':dict[int,int],\n",
    "                        'type':dict[int, str],\n",
    "                        'uri':dict[int,str]\n",
    "})\n",
    "try:\n",
    "    track_schema.validate(dict_trackInfo)\n",
    "except:\n",
    "    error = True\n",
    "    print(\"The track has missing or wrong values\")\n",
    "    logging.error(\"track information doesn't have correct values or is missing values\")\n",
    "if error:\n",
    "    print(\"An error occured here please read the above message and review your data\")\n",
    "else:\n",
    "    print(\"All data passed schemas!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Data Cleaning\n",
    "This was the old way I learned from online, I replaced it with the more efficent and effective schema solution\n",
    "  - Will test the DataFrame for N/A values or empty values and in this situation remove them\n",
    "  - Check and remove duplicate data\n",
    "  - Filter and Shape our data\n",
    "  - Included section for standardization of data but not applicable in current data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks dataframe for any null values OLD STYLE remains as part of proof of learning\n",
    "def cleaner (data):\n",
    "    if(data.isnull().values.any()):\n",
    "        missing_Values = data.isnull().sum().sum() + data['track'].isnull().sum()\n",
    "        logging.warning(missing_Values,\"missing values were found\")\n",
    "        # if the data is a subest of either of the track missing or the input time\n",
    "        if data.isnull().sum().sum() != 0:\n",
    "            count = 0\n",
    "            for item in enumerate(data):\n",
    "                if item[0] or item[1] != 0:\n",
    "                    data.drop([count])\n",
    "                count+=1\n",
    "\n",
    "#Remove outliars and duplicates in the dataset NOT NEEDED for current dataset.\n",
    "if PL:\n",
    "    def clean(data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Cleans the data of outliars and duplicate data.\n",
    "\n",
    "        data: DataFrame of data you want to clean.\n",
    "        \"\"\"\n",
    "        #NDArray of z scores of my data\n",
    "        z = np.abs(stats.zscore(data))\n",
    "\n",
    "        #only keeps the rows within the aboslute value of 3 (aka within 3 standard deviations)\n",
    "\n",
    "        data = data[(z>3).all(axis=1)]\n",
    "\n",
    "        #Now we'll find the last duplicated values and drop them\n",
    "        data.drop_duplicates()\n",
    "\n",
    "# Not all data we have is useful data to us currently I'll say the only data we care about is as follows\n",
    "# We care about the artist name, the song name, and the song tid\n",
    "#First lets clean up our data a bit more...\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to excel\n",
    "Data has been proven clean so now we export it to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistInfo.to_excel('artist.xlsx')\n",
    "albumInfo.to_excel('album.xlsx')\n",
    "trackInfo.to_excel('track.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
