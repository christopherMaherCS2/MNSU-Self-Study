{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Sub-project\n",
    " Created by Christopher Maher\n",
    "### Goal: Create a API cleaning pipeline\n",
    "\n",
    "  This project is to create an API cleaning ETL pipeline that allows a given API to be cleaned based on the following criteria:\n",
    "\n",
    "  * Handling Missing Data\n",
    "  * Dealing with Duplicate Data\n",
    "  * Standardization of Data\n",
    "  * Handling Outliers of Data\n",
    "  * Reshaping Data \n",
    "  * Filtering and Selecting Data\n",
    "\n",
    "This data pipeline will also attempt to fit best practices in ETL pipeline building which means the following:\n",
    "\n",
    "- Error handling\n",
    "- Logging\n",
    "- Testing\n",
    "\n",
    "The pipeline will be built with an attempt to be able to scale however due to the nature of the project will be unneeded and will be an attempt at best practice.\n",
    "\n",
    "#### Project Details\n",
    "\n",
    "  Use the Spotify API to identify trends in users 'saved songs', more commonly known as favorite songs, to identify their preferred listening type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports and variables\n",
    "All imports are listed below as well as versions used.\n",
    "\n",
    "Versions developed on:\n",
    "\n",
    " `pandas: 2.0.0`\n",
    "\n",
    " `logging: 0.5.1.2`\n",
    " \n",
    " `requests: 2.28.2`\n",
    " \n",
    " `json: 2.0.9`\n",
    "\n",
    " `numpy: 1.24.2`\n",
    "\n",
    "Check to current versions in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import time\n",
    "import json\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import spotipy\n",
    "import scipy.stats as stats\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "## .gitignore file containing secure information not to be published, stores as clientID and clientSecret both as strings\n",
    "import keys\n",
    "\n",
    "# Debug\n",
    "DEBUG = False\n",
    "\n",
    "# Proof of learning\n",
    "\n",
    "PL = False\n",
    "\n",
    "#Check versions\n",
    "if DEBUG:\n",
    "    print(pd.__version__)\n",
    "    print(logging.__version__)\n",
    "    print(requests.__version__)\n",
    "    print(json.__version__)\n",
    "    print(np.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval\n",
    "Retrives the data from the following API: Spotify API\n",
    "\n",
    "- This API is a large data collection provided by Spotify that allows multiple data features to be collected. This can range from how music 'feels' based on internal classifications on Spotify to other similar information provided by the API\n",
    "- This API **requires** creditentials so it's important to add your own credientials into it in the variables listed above\n",
    "- Alongside that currently \n",
    "\n",
    "Puts the data into pandas Dataframes for future processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function to extract the first element of the dictionary\n",
    "def extract_first_val(row):\n",
    "    return list(row.values())[0]\n",
    "\n",
    "#Connect to Spotify API as a user\n",
    "try: \n",
    "    sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=keys.clientID,client_secret=keys.clientSecret,redirect_uri='http://localhost:8080'))\n",
    "    logging.info(\"Established API connection\")\n",
    "\n",
    "except :\n",
    "    print(\"Failed to connect to Spotify\")\n",
    "    logging.exception(\"Failed to connect to Spotify API\")\n",
    "    SystemExit\n",
    "\n",
    "\n",
    "results = sp.current_user_saved_tracks(limit=50)\n",
    "\n",
    "#Fun check for if I'm getting results here\n",
    "if DEBUG:\n",
    "    for idx, item in enumerate(results['items']):\n",
    "        track = item['track']\n",
    "        print(idx, track['artists'][0]['name'], \" â€“ \", track['name'])\n",
    "\n",
    "#put the data in dataframe! During this stage I 'unpack' a lot of the information since it's hidden in dictionaries\n",
    "timeAdded = pd.DataFrame(results['items']).drop(['track'],axis=1)\n",
    "\n",
    "trackInformation = pd.DataFrame(pd.DataFrame(results['items'])['track'].apply(lambda x: pd.Series(x)))\n",
    "\n",
    "albumInfo = trackInformation['album'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "#This was a literal pain to figure out, the for some reason changed their entry types to be a list for artists(from a series) then apply my lambda\n",
    "artistInfo = pd.DataFrame(albumInfo['artists'].tolist())[0].apply(lambda x: pd.Series(x))\n",
    "\n",
    "# apply the function to each row of the 'external_urls' column\n",
    "artistInfo['external_urls'] = artistInfo['external_urls'].apply(extract_first_val)\n",
    "\n",
    "albumInfo['external_urls'] = albumInfo['external_urls'].apply(extract_first_val)\n",
    "\n",
    "#Dropping columns I don't need\n",
    "\n",
    "albumInfo.drop('artists', axis=1)\n",
    "\n",
    "#print(type(artistInfo['id'][49]))\n",
    "logging.info(\"Data has been entered into DataFrame\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Data Schemas\n",
    "   This is to allow us to better define what values we're expecting for each column that we created previously we're going to keep them individually now since it allows for easier cleaning, the important bit will be keeping a unique id on them which will be their row #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import Schema\n",
    "#Convert the artistInfo DataFrame to a dictionary so the schema can work. Then create and validate from the schema information\n",
    "dict_artistInfo = artistInfo.to_dict()\n",
    "artist_schema = Schema({'external_urls':dict[int,str],\n",
    "                            'href': dict[int,str],\n",
    "                            'id': dict[int,str],\n",
    "                            'name': dict[int,str],\n",
    "                            'type': dict[int,str],\n",
    "                            'uri': dict[int,str]\n",
    "})\n",
    "try:\n",
    "    artist_schema.validate(dict_artistInfo)\n",
    "except:\n",
    "    print(\"The artist has missing or wrong values\")\n",
    "    logging.error(\"Artist information doesn't have correct values or is missing values\")\n",
    "#Convert the artistInfo DataFrame to a dictionary so the schema can work. Then create and validate from the schema information\n",
    "album_schema = Schema({'album_group':dict[int,str],\n",
    "                        'album_type': dict[int,str],\n",
    "                        'available_markets': dict[int,list],\n",
    "                        'name': dict[int,str],\n",
    "                        'type': dict[int,str],\n",
    "                        'uri': dict[int,str]\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "  - Will test the DataFrame for N/A values or empty values and in this situation remove them\n",
    "  - Check and remove duplicate data\n",
    "  - Filter and Shape our data\n",
    "  - Included section for standardization of data but not applicable in current data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'track'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'track'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Checks dataframe for any null values\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m(data\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39many()):\n\u001b[1;32m----> 3\u001b[0m     missing_Values \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msum() \u001b[39m+\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mtrack\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\n\u001b[0;32m      4\u001b[0m     logging\u001b[39m.\u001b[39mwarning(missing_Values,\u001b[39m\"\u001b[39m\u001b[39mmissing values were found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[39m# if the data is a subest of either of the track missing or the input time\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'track'"
     ]
    }
   ],
   "source": [
    "#Checks dataframe for any null values\n",
    "if(data.isnull().values.any()):\n",
    "    missing_Values = data.isnull().sum().sum() + data['track'].isnull().sum()\n",
    "    logging.warning(missing_Values,\"missing values were found\")\n",
    "    # if the data is a subest of either of the track missing or the input time\n",
    "    if data.isnull().sum().sum() != 0:\n",
    "        count = 0\n",
    "        for item in enumerate(data):\n",
    "            if item[0] or item[1] != 0:\n",
    "                data.drop([count])\n",
    "            count+=1\n",
    "\n",
    "#Remove outliars and duplicates in the dataset NOT NEEDED for current dataset.\n",
    "if PL:\n",
    "    def clean(data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Cleans the data of outliars and duplicate data.\n",
    "\n",
    "        data: DataFrame of data you want to clean.\n",
    "        \"\"\"\n",
    "        #NDArray of z scores of my data\n",
    "        z = np.abs(stats.zscore(data))\n",
    "\n",
    "        #only keeps the rows within the aboslute value of 3 (aka within 3 standard deviations)\n",
    "\n",
    "        data = data[(z>3).all(axis=1)]\n",
    "\n",
    "        #Now we'll find the last duplicated values and drop them\n",
    "        data.drop_duplicates()\n",
    "\n",
    "# Not all data we have is useful data to us currently I'll say the only data we care about is as follows\n",
    "# We care about the artist name, the song name, and the song tid\n",
    "#First lets clean up our data a bit more...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = 'spotify:track:4TTV7EcfroSLWzXRY6gLv6'\n",
    "start = time.time()\n",
    "analysis = sp.audio_analysis(tid)\n",
    "delta = time.time() - start\n",
    "print(json.dumps(analysis, indent=4))\n",
    "print(\"analysis retrieved in %.2f seconds\" % (delta,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
